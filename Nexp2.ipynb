{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81da5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed, read the dataset\n",
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "string.punctuation = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
    "string.punctuation = string.punctuation.replace('.', '')\n",
    "file = open('corpus.txt', encoding = 'utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2954dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "file_nl_removed = \"\"\n",
    "for line in file:\n",
    "  line_nl_removed = line.replace(\"\\n\", \" \")      #removes newlines\n",
    "  file_nl_removed += line_nl_removed\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2da4a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 981\n",
      "The number of tokens is 27361\n",
      "The average number of tokens per sentence is 28\n",
      "The number of unique tokens are 3039\n"
     ]
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) \n",
    "#prints the number of sentences\n",
    "\n",
    "words = nltk.word_tokenize(file_p)\n",
    "print(\"The number of tokens is\", len(words)) \n",
    "#prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\",\n",
    "average_tokens) \n",
    "#prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
    "#prints the number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8908b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [('the', 1630), ('and', 844), ('she', 537), ('of', 508), ('alice', 385)]\n",
      "\n",
      "Most common bigrams:  [(('said', 'the'), 209), (('said', 'alice'), 115), (('the', 'queen'), 65), (('the', 'king'), 60), (('a', 'little'), 59)]\n",
      "\n",
      "Most common trigrams:  [(('the', 'mock', 'turtle'), 51), (('the', 'march', 'hare'), 30), (('said', 'the', 'king'), 29), (('the', 'white', 'rabbit'), 21), (('said', 'the', 'hatter'), 21)]\n",
      "\n",
      "Most common fourgrams:  [(('said', 'the', 'mock', 'turtle'), 19), (('she', 'said', 'to', 'herself'), 16), (('a', 'minute', 'or', 'two'), 11), (('said', 'the', 'march', 'hare'), 8), (('will', 'you', 'wont', 'you'), 8)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower()\n",
    "    sequence = word_tokenize(sentence) \n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence) \n",
    "    bigram.extend(list(ngrams(sequence, 2)))  \n",
    "#unigram, bigram, trigram, and fourgram models are created\n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))\n",
    "def removal(x):     \n",
    "#removes ngrams containing only stopwords\n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in stop_words:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return(y)\n",
    "unigram = removal(unigram)\n",
    "bigram = removal(bigram)\n",
    "trigram = removal(trigram)             \n",
    "fourgram = removal(fourgram)\n",
    "\n",
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
    "print (\"Most common unigrams: \", freq_uni.most_common(5))\n",
    "print (\"\\nMost common bigrams: \", freq_bi.most_common(5))\n",
    "print (\"\\nMost common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5759c988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams with stopword removal and without add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [('alice', 385), ('little', 128), ('one', 101), ('like', 85), ('know', 85), ('would', 83), ('went', 83), ('could', 77), ('thought', 74), ('time', 68)]\n",
      "\n",
      "Most common bigrams:  [(('mock', 'turtle'), 54), (('march', 'hare'), 31), (('thought', 'alice'), 27), (('white', 'rabbit'), 22), (('alice', 'thought'), 13), (('minute', 'two'), 12), (('poor', 'alice'), 11), (('alice', 'could'), 11), (('dont', 'know'), 10), (('poor', 'little'), 9)]\n"
     ]
    }
   ],
   "source": [
    "#stopwords = code for downloading stop words through nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#prints top 10 unigrams, bigrams after removing stopwords\n",
    "print(\"Most common n-grams with stopword removal and without add-1 smoothing: \\n\")\n",
    "unigram_sw_removed = [p for p in unigram if p not in stop_words]\n",
    "fdist = nltk.FreqDist(unigram_sw_removed)\n",
    "print(\"Most common unigrams: \", fdist.most_common(10))\n",
    "\n",
    "bigram_sw_removed = []\n",
    "bigram_sw_removed.extend(list(ngrams(unigram_sw_removed, 2)))\n",
    "fdist = nltk.FreqDist(bigram_sw_removed)\n",
    "print(\"\\nMost common bigrams: \", fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45eb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add-1 smoothing is performed here.\n",
    "            \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])             #add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b651546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n",
      "\n",
      "Most common unigrams:  [[('the',), 0.05598462224968249], [('and',), 0.02900490852298081], [('to',), 0.02478289225277177], [('a',), 0.02155631071293722], [('she',), 0.018467030515223287], [('it',), 0.018089451824391582], [('of',), 0.017471595784848797], [('said',), 0.015892630350461675], [('i',), 0.013764459547592077], [('alice',), 0.013249579514639755]]\n",
      "\n",
      "Most common bigrams:  [[('said', 'the'), 0.0053395713087035016], [('of', 'the'), 0.0033308754354293268], [('said', 'alice'), 0.0029494774848076483], [('in', 'a'), 0.002491799944061634], [('and', 'the'), 0.002059548933357065], [('in', 'the'), 0.0020086958732741743], [('it', 'was'), 0.0019069897531083933], [('to', 'the'), 0.0017798571029011671], [('the', 'queen'), 0.0016781509827353861], [('as', 'she'), 0.0015764448625696051]]\n",
      "\n",
      "Most common trigrams:  [[('the', 'mock', 'turtle'), 0.001143837575064341], [('the', 'march', 'hare'), 0.0006819031697498955], [('said', 'the', 'king'), 0.0006599062933063505], [('said', 'the', 'hatter'), 0.00048393128175799036], [('the', 'white', 'rabbit'), 0.00048393128175799036], [('said', 'the', 'mock'), 0.0004399375288709003], [('said', 'to', 'herself'), 0.0004399375288709003], [('said', 'the', 'caterpillar'), 0.0004179406524273553], [('she', 'went', 'on'), 0.0003959437759838103], [('she', 'said', 'to'), 0.0003959437759838103]]\n",
      "\n",
      "Most common fourgrams:  [[('said', 'the', 'mock', 'turtle'), 0.00043521782652217433], [('she', 'said', 'to', 'herself'), 0.0003699351525438482], [('a', 'minute', 'or', 'two'), 0.0002611306959133046], [('will', 'you', 'wont', 'you'), 0.00019584802193497845], [('said', 'the', 'march', 'hare'), 0.00019584802193497845], [('said', 'alice', 'in', 'a'), 0.00017408713060886974], [('wont', 'you', 'will', 'you'), 0.00015232623928276102], [('the', 'moral', 'of', 'that'), 0.00015232623928276102], [('in', 'a', 'tone', 'of'), 0.00015232623928276102], [('well', 'as', 'she', 'could'), 0.00015232623928276102]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "print (\"Most common unigrams: \", str(ngrams_prob[1][:10]))\n",
    "print (\"\\nMost common bigrams: \", str(ngrams_prob[2][:10]))\n",
    "print (\"\\nMost common trigrams: \", str(ngrams_prob[3][:10]))\n",
    "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaa174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = 'after that alice said the'\n",
    "str2 = 'alice felt so desperate that she was'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91541eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 1:  {1: ('the',), 2: ('said', 'the'), 3: ('alice', 'said', 'the')} \n",
      "String 2:  {1: ('was',), 2: ('she', 'was'), 3: ('that', 'she', 'was')}\n"
     ]
    }
   ],
   "source": [
    "#smoothed models without stopwords removed are used\n",
    "token_1 = word_tokenize(str1)\n",
    "token_2 = word_tokenize(str2)\n",
    "ngram_1 = {1:[], 2:[], 3:[]}   #to store the n-grams formed  \n",
    "ngram_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    ngram_1[i+1] = list(ngrams(token_1, i+1))[-1]\n",
    "    ngram_2[i+1] = list(ngrams(token_2, i+1))[-1]\n",
    "print(\"String 1: \", ngram_1,\"\\nString 2: \",ngram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ad7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_1 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_1[i+1]:      \n",
    "#to find predictions based on highest probability of n-grams  \n",
    "                 \n",
    "            count +=1\n",
    "            pred_1[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_1[i+1].append(\"NOT FOUND\")           \n",
    "#if no word prediction is found, replace with NOT FOUND            \n",
    "            count +=1\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "pred_2 = {1:[], 2:[], 3:[]}\n",
    "for i in range(3):\n",
    "    count = 0\n",
    "    for each in ngrams_prob[i+2]:\n",
    "        if each[0][:-1] == ngram_2[i+1]:\n",
    "            count +=1\n",
    "            pred_2[i+1].append(each[0][-1])\n",
    "            if count ==5:\n",
    "                break\n",
    "    if count<5:\n",
    "        while(count!=5):\n",
    "            pred_2[i+1].append(\"\\0\")\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aaa599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\n",
      "\n",
      "String 1 - after that alice said the-\n",
      "\n",
      "Bigram model predictions: ['queen', 'king', 'gryphon', 'mock', 'hatter']\n",
      "Trigram model predictions: ['king', 'hatter', 'mock', 'caterpillar', 'gryphon']\n",
      "Fourgram model predictions: ['NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND', 'NOT FOUND']\n",
      "\n",
      "String 2 - alice felt so desperate that she was-\n",
      "\n",
      "Bigram model predictions: ['a', 'the', 'not', 'that', 'going']\n",
      "Trigram model predictions: ['now', 'quite', 'a', 'beginning', 'looking']\n",
      "Fourgram model predictions: ['now', 'walking', 'dozing', 'in', 'quite']\n"
     ]
    }
   ],
   "source": [
    "print(\"Next word predictions for the strings using the probability models of bigrams, trigrams, and fourgrams\\n\")\n",
    "print(\"String 1 - after that alice said the-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\\n\" .format(pred_1[1], pred_1[2], pred_1[3]))\n",
    "print(\"String 2 - alice felt so desperate that she was-\\n\")\n",
    "print(\"Bigram model predictions: {}\\nTrigram model predictions: {}\\nFourgram model predictions: {}\" .format(pred_2[1], pred_2[2], pred_2[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
